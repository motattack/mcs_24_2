# –°—É–ø–µ—Ä–∫–æ–º–ø—å—é—Ç–µ—Ä—ã –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö [üîô](https://github.com/motattack/mcs_24_2)

## ‚ö° CUDA [–ö–ª–∞—Å—Ç–µ—Ä](https://cc.dvfu.ru/)
–ë–∞–∑–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ.

### üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥—É–ª—è CUDA:
```shell
module load cuda
```

### üõ†Ô∏è –ö–æ–º–ø–∏–ª—è—Ç–æ—Ä:
```shell
mpic++
```

### üöÄ –ó–∞–ø—É—Å–∫ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Ç–æ–∫–∞—Ö:
```shell
mpiexec -np 
```

### üìñ –ö–Ω–∏–≥–∏:
1. [CUDA C++ Programming Guide](lessons/books/CUDA_guide.pdf)
2. [–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞ GPU - –ë–æ—Ä–µ—Å–∫–æ–≤](lessons/books/Boreskov_Parallelnye-vychisleniya-na-GPU-Arhitektura-i-programmnaya-model-CUDA.pdf)
3. [CUDA. API - –†–æ–º–∞–Ω–µ–Ω–∫–æ](lessons/books/4ICaG_2.pdf)
4. [–û—Å–Ω–æ–≤—ã —Ä–∞–±–æ—Ç—ã —Å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–µ–π CUDA - –ë–æ—Ä–µ—Å–∫–æ–≤](lessons/books/Boreskov_A_V_,_Kharlamov_A_A._Introduction_to_CUDA_Technology_2010.pdf)
5. [–ü—Ä–æ–≥—Ä–∞–º–º–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ CUDA - –ö–∞–ª–≥–∏–Ω](lessons/books/cuda-2-program-arch.pdf)
6. [–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è CUDA –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö - –°–∞–Ω–¥–µ—Ä—Å, 2013](lessons/books/Sanders_J_,_Kandrot_E._CUDA_by_Example_2013.pdf)
7. [üá¨üáß CUDA Programming - Cook](lessons/books/Shane_Cook_‚Äî_CUDA_Programming_A_Developer's_Guide_to_Parallel_Computing.pdf)
8. [üá¨üáß Deep Belief Nets in C++ and CUDA C: Volume 3 - Timothy](lessons/books/Masters_T_Deep_Belief_Nets_in_C++_and_CUDA_C_Volume_3_Convolutional.pdf)
9. [üá¨üáß Parallel Computing for Data Science - Norman](lessons/books/Norman_Matloff_Parallel_Computing_for_Data_Science_With_Examples.pdf)
10. [üá¨üáß CUDA Application Design and Development - Farber](lessons/books/Rob_Farber_‚Äî_CUDA_Application_Design_and_Development_‚Äî_2011.pdf)
11. [üá¨üáß PROFESSIONAL CUDA C Programming - Cheng](lessons/books/Cheng_J_,_Grossman_M_,_McKercher_T_Professional_CUDA_C_Programming.pdf)
---


## üìù –ó–∞–¥–∞–Ω–∏–µ 1
–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–¥:

```cu
// —Ñ–∞–π–ª: "hw.cu"
#include <iostream>

int main(int argc, char ** argv) {
  std::cout << "Hello, world!\n";

  return 0;
}
```

–ê —Ç–∞–∫ –∂–µ:

```cu
// —Ñ–∞–π–ª: "hw_d.cu"
#include<iostream>
#include<stdio.h>
#include<cuda_runtime.h>

using namespace std;

__global__ void kernel(void) {
  int ID = blockIdx.x * blockDim.x + threadIdx.x;

  if (ID == 9) {
    printf("blockDim.x %d\n", blockDim.x);
    printf("Hello from block %d, thread %d\n", blockIdx.x, threadIdx.x);
    printf("identificator, or number of thread is %d\n", ID);
  }

  /*	if(ID==10)
  	{	
        printf("Hello from block %d, thread %d\n", blockIdx.x, threadIdx.x);
        printf("Hello from theread %d\n", ID);
      }
  */
}

int main(void) {
  cout << "Hello, CUDA! \n"; //cpu

  kernel << < 2, 10 >>> (); //gpu 

  cudaDeviceSynchronize();

  return 0;
}
```

---

## üìù –ó–∞–¥–∞–Ω–∏–µ 2
–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ª–æ–∂–µ–Ω–∏–µ –¥–≤—É—Ö –º–∞—Å—Å–∏–≤–æ–≤ –Ω–∞ GPU:

```cu
// Kernel definition
global void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}
int main()
{
    ...
    // Kernel invocation with N threads
    VecAdd<<<1, N>>>(A, B, C);
    ...
}
```

üìå **–ó–∞–º–µ—á–∞–Ω–∏–µ:**
```cu
int i = blockIdx.x * blockDim.x + threadIdx.x;
```

–°–º. –∫–Ω–∏–≥—É ‚Ññ1. –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π.

---

### üìò –ü—Ä–∏–º–µ—Ä —Ä–µ—à–µ–Ω–∏—è:

```cu
// —Ñ–∞–π–ª: "vecadd_d.cu"
#include <iostream>

#include<stdio.h>

#include <cuda_runtime.h>

#define N 10

__global__ void VecAdd(float * A, float * B, float * C) {
  int i = threadIdx.x;
  C[i] = A[i] + B[i];
}

int main() {
  float * A = new float[N];
  float * B = new float[N];
  float * C = new float[N];

  for (int i = 0; i < N; ++i) {
    A[i] = i;
    B[i] = i * 2.0;
  }

  float * dev_A, * dev_B, * dev_C;
  cudaMalloc((void ** ) & dev_A, sizeof(float) * N);
  cudaMalloc((void ** ) & dev_B, sizeof(float) * N);
  cudaMalloc((void ** ) & dev_C, sizeof(float) * N);

  cudaMemcpy(dev_A, A, sizeof(float) * N, cudaMemcpyHostToDevice);
  cudaMemcpy(dev_B, B, sizeof(float) * N, cudaMemcpyHostToDevice);

  // Kernel invocation with N threads
  VecAdd << < 1, N >>> (dev_A, dev_B, dev_C);

  cudaDeviceSynchronize();

  cudaMemcpy(C, dev_C, sizeof(float) * N, cudaMemcpyDeviceToHost);

  for (int i = 0; i < N; ++i) {
    std::cout << C[i] << "\n";
  }

  delete[] A;
  delete[] B;
  delete[] C;

  cudaFree(dev_A);
  cudaFree(dev_B);
  cudaFree(dev_C);
}
```

---

## üìù –ó–∞–¥–∞–Ω–∏–µ 3
–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ª–æ–∂–µ–Ω–∏–µ –¥–≤—É—Ö 2-–º–µ—Ä–Ω—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤ –Ω–∞ GPU.

```cu
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}

```

–°–º. –∫–Ω–∏–≥—É ‚Ññ1. –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π.

---

## üìù –ó–∞–¥–∞–Ω–∏–µ 4
–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –º–æ–¥–µ—Ä–Ω–∏–∑–æ–≤–∞—Ç—å –∫–æ–¥ –∏–∑ –∑–∞–¥–∞–Ω–∏—è 3,
—á—Ç–æ–±—ã –æ–Ω —Ä–∞–±–æ—Ç–∞–ª –≤ —Å—Ç–∏–ª–µ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.

–°–º. –∫–Ω–∏–≥—É ‚Ññ1. –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π.

---

## üìù –ó–∞–¥–∞–Ω–∏–µ 5
–ö–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª.
–ó–∞–ø–æ–ª–Ω—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–º–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ int —á–∏—Å–ª–∞–º–∏
–æ—Ç 0 –¥–æ 10 000 000 –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤ –Ω–∞ GPU.
–í—Ç–æ—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –Ω–∞–π—Ç–∏ –º–∏–Ω–∏–º—É–º.